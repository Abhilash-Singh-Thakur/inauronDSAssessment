{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c311e97a",
   "metadata": {},
   "source": [
    "1. Working with RDDs:\n",
    "   a) Write a Python program to create an RDD from a local data source.\n",
    "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993aa4f",
   "metadata": {},
   "source": [
    "a) Write a Python program to create an RDD from a local data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d352dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(appName=\"RDDExample\")\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf23a3",
   "metadata": {},
   "source": [
    "b) Implement transformations and actions on the RDD to perform data processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: map - Multiply each element by 2\n",
    "rdd_mapped = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Transformation: filter - Filter even numbers\n",
    "rdd_filtered = rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Action: reduce - Sum all elements\n",
    "rdd_sum = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Action: collect - Retrieve the RDD elements as a list\n",
    "rdd_list = rdd.collect()\n",
    "\n",
    "# Action: count - Count the number of elements in the RDD\n",
    "rdd_count = rdd.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3561c",
   "metadata": {},
   "source": [
    "c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: map - Square each element\n",
    "rdd_squared = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Transformation: filter - Filter values greater than 3\n",
    "rdd_filtered = rdd.filter(lambda x: x > 3)\n",
    "\n",
    "# Action: reduce - Calculate the product of all elements\n",
    "rdd_product = rdd.reduce(lambda x, y: x * y)\n",
    "\n",
    "# Action: aggregate - Calculate the sum and count of elements\n",
    "(rdd_sum, rdd_count) = rdd.aggregate((0, 0),\n",
    "                       (lambda acc, value: (acc[0] + value, acc[1] + 1)),\n",
    "                       (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489c99d",
   "metadata": {},
   "source": [
    "2. Spark DataFrame Operations:\n",
    "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213971a",
   "metadata": {},
   "source": [
    "a) Write a Python program to load a CSV file into a Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view for the DataFrame\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Execute a SQL query on the DataFrame\n",
    "result_df = spark.sql(\"SELECT name, age FROM people WHERE age > 30\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6adcb",
   "metadata": {},
   "source": [
    "b)Perform common DataFrame operations such as filtering, grouping, or joining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered_df = df.filter(df[\"age\"] > 30)\n",
    "\n",
    "# Group by a column and calculate average\n",
    "average_age_df = df.groupBy(\"city\").avg(\"age\")\n",
    "\n",
    "# Join two DataFrames\n",
    "joined_df = df.join(other_df, df[\"id\"] == other_df[\"id\"], \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839ae0c",
   "metadata": {},
   "source": [
    "c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view for the DataFrame\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Execute a SQL query on the DataFrame\n",
    "result_df = spark.sql(\"SELECT name, age FROM people WHERE age > 30\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a40c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Spark Streaming:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a) Write a Python program to create a Spark Streaming application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ef89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "# Set the log level to avoid excessive logging\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hostname and port for the socket source\n",
    "hostname = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "# Create a DStream by connecting to the socket source\n",
    "lines = ssc.socketTextStream(hostname, port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Implement streaming transformations and actions to process and analyze the incoming data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81847f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: flatMap - Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "\n",
    "# Transformation: filter - Filter words starting with 'a'\n",
    "filtered_words = words.filter(lambda word: word.startswith('a'))\n",
    "\n",
    "# Action: countByValue - Count the occurrences of each word\n",
    "word_counts = filtered_words.countByValue()\n",
    "\n",
    "# Output the word counts\n",
    "word_counts.pprint()\n",
    "# Transformation: flatMap - Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "\n",
    "# Transformation: filter - Filter words starting with 'a'\n",
    "filtered_words = words.filter(lambda word: word.startswith('a'))\n",
    "\n",
    "# Action: countByValue - Count the occurrences of each word\n",
    "word_counts = filtered_words.countByValue()\n",
    "\n",
    "# Output the word counts\n",
    "word_counts.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Spark SQL and Data Source Integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98895710",
   "metadata": {},
   "outputs": [],
   "source": [
    "a) Write a Python program to connect Spark with a relational database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a44a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLExample\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"jdbc_driver.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure the database connection\n",
    "database_url = \"jdbc:mysql://localhost:3306/database_name\"\n",
    "database_properties = {\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "# Read data from the database into a DataFrame\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", database_url) \\\n",
    "    .option(\"dbtable\", \"table_name\") \\\n",
    "    .option(\"properties\", database_properties) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f64657",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) Perform SQL operations on the data stored in the database using Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ae308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view for the DataFrame\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Execute SQL queries on the DataFrame\n",
    "result_df = spark.sql(\"SELECT * FROM my_table WHERE column_name > 10\")\n",
    "result_df.show()\n",
    "# Create a temporary view for the DataFrame\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Execute SQL queries on the DataFrame\n",
    "result_df = spark.sql(\"SELECT * FROM my_table WHERE column_name > 10\")\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b482a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Explore the integration capabilities of Spark with other data sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19225ac",
   "metadata": {},
   "source": [
    "Spark provides integration capabilities with various data sources, such as HDFS or Amazon S3. You can use the appropriate file system connectors to read and write data from these sources. Here's an example of reading data from HDFS using Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09915b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from HDFS into a DataFrame\n",
    "df_hdfs = spark.read.csv(\"hdfs://localhost:9000/path/to/file.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ff730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
