{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48286e21",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Create a ConfigParser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config.read('/path/to/hadoop/conf/core-site.xml')\n",
    "\n",
    "# Get the core components from the configuration file\n",
    "core_components = config['default']['fs.defaultFS']\n",
    "\n",
    "# Display the core components of Hadoop\n",
    "print(\"Core Components of Hadoop:\")\n",
    "print(core_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5e6da",
   "metadata": {},
   "source": [
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_directory_size(hdfs_host, hdfs_port, directory_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(f\"http://{hdfs_host}:{hdfs_port}\")\n",
    "\n",
    "    # Get a list of files in the directory\n",
    "    file_list = client.list(directory_path, status=True)\n",
    "\n",
    "    # Calculate the total file size\n",
    "    total_size = sum(status['length'] for path, status in file_list if not status['type'])\n",
    "\n",
    "    # Return the total file size\n",
    "    return total_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1ce9c",
   "metadata": {},
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21775b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--N', type=int, default=10, help='Number of top words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_topN_words)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_topN_words(self, _, word_count_pairs):\n",
    "        N = self.options.N\n",
    "        topN_words = heapq.nlargest(N, word_count_pairs)\n",
    "        for count, word in topN_words:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c6fc1",
   "metadata": {},
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3715277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Hadoop cluster URL and port\n",
    "hadoop_url = 'http://your_hadoop_cluster_url'\n",
    "hadoop_port = '50070'  # NameNode port\n",
    "\n",
    "# Check NameNode health\n",
    "namenode_url = f'{hadoop_url}:{hadoop_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus'\n",
    "namenode_response = requests.get(namenode_url).json()\n",
    "namenode_status = namenode_response['beans'][0]['State']\n",
    "\n",
    "print('NameNode Status:', namenode_status)\n",
    "\n",
    "# Check DataNode health\n",
    "datanode_url = f'{hadoop_url}:{hadoop_port}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-*'\n",
    "datanode_response = requests.get(datanode_url).json()\n",
    "datanode_states = datanode_response['beans']\n",
    "\n",
    "print('DataNode Status:')\n",
    "for datanode_state in datanode_states:\n",
    "    datanode_name = datanode_state['name']\n",
    "    datanode_status = datanode_state['VolumeInfo']\n",
    "    print(f'{datanode_name}: {datanode_status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6776e",
   "metadata": {},
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "def list_hdfs_path_files_and_directories(hdfs_path):\n",
    "    # Connect to the HDFS file system\n",
    "    fs = hdfs.connect()\n",
    "\n",
    "    # List all files and directories in the given HDFS path\n",
    "    files_and_dirs = fs.ls(hdfs_path)\n",
    "\n",
    "    # Print the list of files and directories\n",
    "    for item in files_and_dirs:\n",
    "        print(item)\n",
    "\n",
    "# Specify the HDFS path to list\n",
    "hdfs_path = '/path/to/hdfs/directory'\n",
    "\n",
    "# Call the function to list files and directories\n",
    "list_hdfs_path_files_and_directories(hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e3f35",
   "metadata": {},
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_data_node_storage_utilization():\n",
    "    # Execute Hadoop CLI command to retrieve DataNode storage information\n",
    "    command = \"hdfs dfsadmin -report\"\n",
    "    output = subprocess.check_output(command.split()).decode()\n",
    "\n",
    "    # Parse the output to extract DataNode storage information\n",
    "    data_nodes = output.split(\"Name: \")[1:]\n",
    "    storage_utilization = {}\n",
    "\n",
    "    # Extract storage information for each DataNode\n",
    "    for node in data_nodes:\n",
    "        lines = node.split(\"\\n\")\n",
    "        name = lines[0]\n",
    "        storage = [line.split(\":\") for line in lines[1:] if \"DFS Used%\" in line]\n",
    "        storage_utilization[name] = int(storage[0][1].strip())\n",
    "\n",
    "    return storage_utilization\n",
    "\n",
    "# Get storage utilization of DataNodes\n",
    "storage_utilization = get_data_node_storage_utilization()\n",
    "\n",
    "# Identify the node with the highest and lowest storage capacities\n",
    "highest_capacity_node = max(storage_utilization, key=storage_utilization.get)\n",
    "lowest_capacity_node = min(storage_utilization, key=storage_utilization.get)\n",
    "\n",
    "# Print the results\n",
    "print(\"Node with the highest storage capacity:\", highest_capacity_node)\n",
    "print(\"Node with the lowest storage capacity:\", lowest_capacity_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df22178",
   "metadata": {},
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yarn_api_client import ApplicationMaster, HistoryServer, ResourceManager\n",
    "from time import sleep\n",
    "\n",
    "def submit_hadoop_job(jar_path, main_class, input_path, output_path):\n",
    "    # Connect to the YARN ResourceManager API\n",
    "    resourcemanager = ResourceManager()\n",
    "\n",
    "    # Submit the Hadoop job\n",
    "    application_id = resourcemanager.submit_new_application(jar_path, main_class, input_path, output_path)\n",
    "    print(\"Job submitted. Application ID:\", application_id)\n",
    "\n",
    "    return application_id\n",
    "\n",
    "def monitor_job_progress(application_id):\n",
    "    # Connect to the YARN ResourceManager API\n",
    "    resourcemanager = ResourceManager()\n",
    "\n",
    "    # Monitor the job progress\n",
    "    while True:\n",
    "        status = resourcemanager.application_status(application_id)\n",
    "        print(\"Job status:\", status['state'])\n",
    "\n",
    "        if status['state'] in ['FINISHED', 'FAILED', 'KILLED']:\n",
    "            break\n",
    "\n",
    "        sleep(10)\n",
    "\n",
    "def retrieve_job_output(application_id):\n",
    "    # Connect to the YARN HistoryServer API\n",
    "    historyserver = HistoryServer()\n",
    "\n",
    "    # Retrieve the final output of the job\n",
    "    output = historyserver.get_job_output(application_id)\n",
    "    print(\"Job output:\", output)\n",
    "\n",
    "# Specify the Hadoop job details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e5317",
   "metadata": {},
   "source": [
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8819ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yarn_api_client import ApplicationMaster, ResourceManager\n",
    "from time import sleep\n",
    "\n",
    "def submit_hadoop_job(jar_path, main_class, input_path, output_path, num_executors, executor_memory, executor_cores):\n",
    "    # Connect to the YARN ResourceManager API\n",
    "    resourcemanager = ResourceManager()\n",
    "\n",
    "    # Set resource requirements for the Hadoop job\n",
    "    resource = {\n",
    "        \"memory\": executor_memory,\n",
    "        \"vCores\": executor_cores\n",
    "    }\n",
    "\n",
    "    # Submit the Hadoop job with resource requirements\n",
    "    application_id = resourcemanager.submit_new_application(jar_path, main_class, input_path, output_path,\n",
    "                                                            resource, num_executors)\n",
    "    print(\"Job submitted. Application ID:\", application_id)\n",
    "\n",
    "    return application_id\n",
    "\n",
    "def monitor_resource_usage(application_id):\n",
    "    # Connect to the YARN ApplicationMaster API\n",
    "    applicationmaster = ApplicationMaster(application_id)\n",
    "\n",
    "    # Monitor resource usage during job execution\n",
    "    while True:\n",
    "        resource_usage = applicationmaster.resource_usage()\n",
    "        print(\"Resource usage:\", resource_usage)\n",
    "\n",
    "        if resource_usage['state'] in ['FINISHED', 'FAILED', 'KILLED']:\n",
    "            break\n",
    "\n",
    "        sleep(10)\n",
    "\n",
    "# Specify the Hadoop job details and resource requirements\n",
    "jar_path = '/path/to/hadoop-job.jar'\n",
    "main_class = 'com.example.hadoopjob.Main'\n",
    "input_path = '/path/to/input'\n",
    "output_path = '/path/to/output'\n",
    "num_executors = 5\n",
    "executor_memory = \"2g\"\n",
    "executor_cores = 2\n",
    "\n",
    "# Submit the Hadoop job with resource requirements\n",
    "application_id = submit_hadoop_job(jar_path, main_class, input_path, output_path, num_executors, executor_memory, executor_cores)\n",
    "\n",
    "# Monitor resource usage during job execution\n",
    "monitor_resource_usage(application_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f3086",
   "metadata": {},
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ecfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class WordCountJob(MRJob):\n",
    "    def configure_args(self):\n",
    "        super(WordCountJob, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', type=int, default=64,\n",
    "                              help='Input split size in megabytes')\n",
    "\n",
    "    def job_runner_kwargs(self):\n",
    "        kwargs = super(WordCountJob, self).job_runner_kwargs()\n",
    "        kwargs['hadoop_extra_args'] = ['-D', 'mapreduce.input.fileinputformat.split.maxsize=' + str(self.options.split_size * 1024 * 1024)]\n",
    "        return kwargs\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "# Specify the input file path\n",
    "input_file = '/path/to/input.txt'\n",
    "\n",
    "# Specify the different split sizes to compare\n",
    "split_sizes = [32, 64, 128]  # in megabytes\n",
    "\n",
    "# Run the job with different split sizes and measure execution time\n",
    "for split_size in split_sizes:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Construct the command-line arguments for the job\n",
    "    args = ['--split-size', str(split_size), input_file]\n",
    "\n",
    "    # Run the job\n",
    "    job = WordCountJob(args=args)\n",
    "    with job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Print the execution time for each split size\n",
    "    print(f\"Split Size: {split_size} MB, Execution Time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69b663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
